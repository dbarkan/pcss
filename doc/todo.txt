goals of parameter refactoring:
break up internal into sections and clean it
try to get rid of extra config files lying around
maybe try to clean up the disopred command involving uniprot and blast
document where all config files are and how they work in different contexts
possibly hard-code internal config file somewhere -- right now requring a user_pcss_directory which I would rather avoid (but maybe unavoidable?)
think pdh getbenchmarkscorefile is ok for now but decide if I want to keep it that way (different parameter names depending on stand-alone vs server)
finalize cluster config files with minimum params required

training benchmark:
move svm_training_c and svm_training_gamma to internal

there are a couple extra parameters on the cluster node config files that don't need to be there, e.g. job_directory, could clean that up


Next:
organize todo list
training annotation
traning benchmark
user model
massive parameter refactoring
test disopred long on server

have frontend mismatch keyword match backend (should be peptideSequenceMismatch, not mismatch)
have server throw intentional error on the node and make sure it is properly picked up by FinalizeRunner
definitely broke all cluster functionality, esp training benchmark -- put it back in
see if I can factor out some of the prepare training application / svm application / server / cluster runner stuff which all have similarities in their pipelines
need to add training annotation config in server -- not there yet (see what svm application server needs and use that as basis)
also trianing benchmark, and remove server config if possible)

pretty big issue -- right now svmApplicationConfig in config/server sets pcss_directory and run_directory
BUT that should be dynamically set. The problem is that it also has %(pcss_directory)s already.
could either move everything to internal config or come up with some other solution.
for now have sub-optimal solution wher I am setting pcss_directory and run_directory again in svmApplicationConfig

next:
think I can install pcss into /netapp/sali/peptide/pcss
try that


< can I merge config files for cluster and server?
< also see how much I can push from user config into internal config for everything
39,41d32
< maybe a general analysis about what I like about how I have set things up and what I don't like, either change what I don't like
< or learn some lessons from it.
<    also talk with Ben to get some ideas

prepare server and finalize server in new init
finalize run (state file, sge_options, task list)
new init into pcss test

process logic for user custom model


issues with directory:
essentially have six different places where things are run
trombone
trombone test
netapp
netapp test
server
server test

1. expected output will break where trombone is hardcoded
solution: have output be written as %(pcss_directory)s -- needs to be a config file somehow
then have top level pcss_directory be read from somewhere else (maybe)

2. uniprot is its own file, doesn't go in pcss directory structure
could have it be set in a global config file (maybe -- don't like having too many globals)

quick scan through pcssTools and pcssCluster to take out extra code





things to look at:
getRunName has strip at the end, not as clean
move all getSeqBatchDirectory to pdh
do i have regression test expected output for cluster benchmark prepare and finalize? if not write one and also make sure it works as expected
should probably run all of this on the cluster again to test
consider having pcss_directory written by config file generator, otherwise writing it twice (once in base config, once in internal config)
	 see if there are other paramters this applies to
see if I can factor out prepareServerRunner vs prepareClusterRunner and have only differences be at pdh level


p
NEXT

Continue profiling depending on what Josh says
assess results of full proteome
run training with profiler / large dataset
continue with __init__ on modbase site

determine utility of using config spec with server input; if we use it, take out "using_web_server" explicit search for false string
big refactoring of job_directory / run_name stuff

un-essential refactoring and cleaning
memory profile

either run all psipred / disopred with standalone or run as part of big dataset
       if get timeout, will have logs and can adjust accordingly

clean up current web service, start test
integrate into backend
create regression web backend



MEMORY PROFILE
qstat -j $JOB_ID > $OUTPUT_DIR/$input.memory_txt
not working-- reports 0. email Josh.
for both svmApplication and training
   figure out what the command is to get max memory -- should be in docking
   first runs-- ask for extra memory; more than 1GB for sure
training: do small number of peptides a small number of iterations
	  do large number of peptides a small number of iterations
	  etc.

svm application
    do small number of peptides
    do larger number of peptides
    more controlled by seq batch than peptide size so should be safer

CLEANING
clean bin directory?
clean training run directory?
I think the cluster run scripts can be used for command line run scripts too; consider moving and making universal
remove model_directory param everywhere

RENAMING
TrainingSvm / TrainingBenchmarker -- make names consistent with everything elesn
check old server to make sure seqBatchDirectory and seqBatchFilePrefix are the same
check old server to see if seq batch subdirectories had prefixes (1, 2, etc.)
change annotationOutput.txt to be more context specific
rename finalizeApplciationClusterRunner script
change all runner that prepare cluster to have 'prepare' in their run name
maybe change disopred standalone config spec param name back to user_standalone_config_spec

BIG DATASET
train a model based on random peptides or maybe grb peptides
apply it to human proteome
check current memory requested requirements, make sure I am not exceeeding that

PRODUCTION
Run big dataset through
make sure it works out of the box -- maybe clean install on mac
document installation and test procedure
document basic functionality -- order to run scripts
consider how to make accessible -- git?
compare user created model with old code vs new
enough logging to get close to where it was before. Think the user really only cares about frontend stuff so should be close already
webserver backend
add new human models to model table

webserver front end and refactor that too
see if old version reports critical rate in final training file -- didn't see it in backend (maybe in LOO?)
have DefinedPeptideImporter get max peptide length from input
     frontend was doing this before, backend a better place? scan user has to set though, but might be OK if not symmetrical...think about it
if running with SVM Annotation Input mode, need to link the fasta file for sequences with the original fasta file that was passed. 
   functionality will be different depending on whether fasta file was scan or defined -- could be time to make specialized fasta readers
   don't think I had to do this before since I was alwyas essentailly svm annotaiton features mode

ERROR HANDLING
change validate to say what exactly failed -- look at ptPhage
when reading existing error, make sure the message (line 1) is printed out from the previous error -- might be printing the error type (line 0)
leave one out -- keep track of tested peptides and throw error if they were already tested once 
check to make sure fpr for one test set result is always >= fpr in the next line
check to make sure tp is always greater than tp in previous line
in annotation mode, try to throw a global exception while annotation file is open to make sure it closes file correctly
validate runinfo
validate pcssFileAttributes
take out exception in getModelStyle,
have tests for problems in rules file - same as in frontend
fix test_sequence_long -- produces different result for the long sequence than previous (need to see if this is the same result on cluster)
make sure files are always closed
     also make sure these are closed when handling exception
take out pcss runner check for config spec -- should always be there

WEB SERVER 
* update job directory logic when get to web server; think we might have to change it across different steps of the server (preprocess vs run)
* factor out cluster header commands using on_web_server param after seeing what web server provides
* create good benchmark situation for web server regression
* run through svm application mode and see if we get same or similar scores before and after
* also consider outputting these to annotation file. this will necessitate checking to make sure they aren't processed at each step
* maybe process sequences in increasing size. That way a long sequence won't time out and kill it for the rest of the sequences
* If one long sequence does kill it for the rest of the sequences (regardless of whether they were sorted in increasing size),
  see what the output looks like -- are there just feature errors for everything that didn't get processed but only one error 
  actually saying what happened for the one sequence? Could have an attribute for pcssProteins that says if the algorithm started 
  to run, that way we would know if it ran and had an error vs never ran because something else timed out. 
* In pcssFeatureHandler, make sure timeout feature exception message will work (the problem is that on the cluster, might be
  raising exceptions but they don't get caught because the whole code that is also processing exceptions dies during the timeout)
* see if I did anything with the peptides that said keyword sequence mismatch other than write to log file
* copy only certain files back to disk
* double check logic for making sure there are more negatives than positives after mismatch -- should be fine though, also have exception on back end
* put in list of identical peptides
* can't remember if I am requireing the user to have a keyword for application defined mode. if not, don't, and do it myself when I create the fasta file


REFACTORING
modelTable logic (with PcssModel and PcssModelSequence) is a little confusing, see if I can clean it up -- good practice for databases
in training mode, dont' need peptide importer type parameter
consider having only one inpout file parameter for training svm modes and application modes, will cut down on number of parameters. might not b epossible though.
pcssTest has getProtein method, is that needed anywhere else? a little clunky
on cluster, have something dynamically generate file names for stdout / script / etc. instead of having duplicates in parameter file
consider refactor of both training benchmarkers to take peptide and handle iteration counts themselves
consider consolidating loo trainer and jackknife trainer, use parameter to switch between them
consider moving svm classify command and svm learn command from internal to pcssConfig to avoid having to keep pcss_directory in that file
change all the 'with self.assertRaises' code in test which is three lines to one line, factoring out method (after stupidly changing it to this format)
all internal config files should have explicit method from runner for retrieval rather than having whatever needs it being able to access the runner's internal config
    esp since they are usually in a special directory
take out instance data for pcesProtein, pcssPeptide - have it all be getModbaseSequenceId() and then goes to the attribute dictionary
change disorder variable names to disopred
have specialized fasta readers to parse fasta header
can have these specialised fasta readers check to make sure all proteins have sequences instead of way I'm currently doing it
getTwoLetterDir and getThreeLetterDir return differnt directory structures; synergize them
have modelattribute types all be string atttribute objects, then have PCssIo not have to do checks for model string attributes as "" in setValueFromFile
keyword for no errors instead of hardcoding "none", same for no peptide parsed
isError in pcssIo shouldn't be hardcoded
right now we tell a feature it has an error by setting it to be a string attribute with an error prefix value. Better way would be to have a flag for that feature that says if it's an error. Can have the flag be on the superclass. when reading from attribute file, would have to then initialize the feature as its own class with the flag set rather than as a string attribute which is what is happening now.
change references to model to be homology model and svm model to be svm model to distinguish them
have pcss check for operating system and run svm type accordingly
have every pcss global exception have an error code that the test can check to make sure the right error is being thrown -- will have a lot, consider if worth it
     or have them all be pcss global exception subclasses


TEST REFACTOR
create cluster test and seq batch directories; test InternalException filename (they are the only ones who throw it)
finish test for training svm runner (currently just runs but doesn't check anything)
change run name from develop
maybe methods for getting input instead of hardcoding them
change testFileOutput to testData
test for training benchmarker
think of a way to streamline reading input annotation. Tough because different things have different runners.
      also see if can streamline runner creation
      can grep for testInput, can help to see what reads annotation      
svmAppFeatures and svmAppInput both have same expected output, is that supposed to be the case?
move all needed input to test directory -- especially internal config (seq batch count will cause trouble)

TESTING GENERAL
have a test for error exists exception
testing for run disopred standalone and cluster
test for exception if not set peptide length in pcssTools (might be tough)
test for exception in makepeptidefromcode
have checks to make sure run_disopred_command has all files it needs. Split the command, and check for existence of the command itself and everhting after
same for psipred
had a case where I put the wrong run_disopred_Command lcoation in and the internalErrors.out wrote "no such file or directory" -- possible to get the actual name of the file it is looking for and put that in the error output too?
have test to make sure no peptide in training set is in the test set for all benchmarkers
consider having one feature error of each type in annotation output and anything that reads in an annotation file for all normal / runner processing 
	 just need dssp, disopred, psipred -- working on disopred now
         disopred error -- changed file ffef0e2dd621d6a5fc2851aef4341226MESAYTRS|Q16572 -- in disopred file changed position 484 from V (wildtype) to R (induced error)
	 peptides different length -- sequence ffac1dc8985f57c80dd0b64a519b9adbMPNFVENT peptide at position 23 (FPRDPAR) has 7 residues instead of 8. this is only in test input
	 svmTrainingAnnotationInput
have error test to make sure all peptides have feature error when the first one gets it
have test with annotaiton defined, one peptide that is different lengths than others
have test for exception in pcssSvm.finalizeFeature()

TESTING CLUSTER
can't do exact file line comparison in dynamically ge
test for correct number of directories made by seq batch splitter
test two exceptions in mergeSvmApplicationResutls -- will have to figure out how to do seq batch directory structure (can do when refactoring tests)
make sure we have column row as first row
maybe check to make sure we got a result for all input
use annotatin file writer to get columns header?
change annotation file writer to write to the file that I give it. right now jus changing its ouptut fh. 
read back in sequences that were written to different directories and test they are 1:1 with what was originally written
have something that says if output directory isn't set, throw error -- need to figure out at which point to do that
test "del pcssCopy["run_name"]" in pcssCluster.py -- cases where it does and does not have
everywhere --- have recently added a lot of os.mkdir. Might need to add sleep until done.
review previous cluster merging / seq batch code, especially errors (although deprioritize, think I can just start de novo or find errors in big datasets since so much has changed)

CODE
psipred standalone runner
Consider having gaps as input in peptides
 make a python script to make fasta defined input from normal input file. For now can just use perl script, and factor it out from db
 rename peptide and protein basic attributes to be standardized; have try to take out all the input people having to say 'proteins.values()' or change to getProteinDict()
 make sure peptides know their proteins in all instances -- allows for better error writing as well as benchmark reading / writing 
 have peptide start position and protein be written out during leave one out
consider interpolating at critical point, see how web server does is
do context with peptides read in memory and then immediately to training / test -- do some profiling to see how long it takes
If I have an error for a feature for one peptide, it applies for all peptides in the protein. should I keep it that way or have it be specific to that peptide only?
attribute file names / locations shold be internal keywords (maybe)
consider having sorted peptide list in ClassifySvm be finalized or constant so it can't be resorted, or some other way to keep those peptides
	 sorted in that order
explicit vs implicit zeroes for sequence feature

soon will have to come up with parameters for different contexts. e.g. application SVM should have model file  / benchmark scores as config (and there need to decide internal vs user); training should have them as output (probably internal). Same thing for annotation output file
move annotationOutput file to internal cofnig
total model count
template pdb
* other features (model url)
change acc call to letters
change secondary structure styel to be like amino acids: three features per call, set the feature that is my call to 1


KNOWN ERRORS

LARGER DATA:
make sure bestModel still works when have multiple models (currently sorted works, but only two models)
would be great to see if current svm model is the same as a previous model in the perl code
* See what is using 1GB when run on 80 seqs -- see if I can find profiler

ALL
dynamically find config spec file in all runners or executables the same way I am doing it for cluster code

LOGGING
be very explicit about config file errors; make sure that the actual config file that failed is named (internal vs user)
note explicitly where there is a peptide mismatch 

SEQUENCE FEATURES:
explore using different blast database besides nr (maybe nr95?)
psipred and disopred standalone script (probably cluster) and SOP

PLOW
git commit
logging / stats
go through peptide pipeline to see if i missed anything

LIST OF CONTEXTS
CONTEXT -- Normal Annotation no input, write output (AnnotationRunner)
	Scan or Defined input. Scan needs input fasta file; defined needs input fasta file and list of peptide codes
	SUB CONTEXT: Single CPU
	SUB CONTEXT: Cluster command line


CONTEXT -- Annotation and SVM Application no input, write output (SvmApplicationRunner) -- features are both input and output and svm is output only
	Input is Defined or scan input mode. Web server in defined mode will dynamically generate input file from defined file
	SUB CONTEXT: Single CPU
	SUB CONTEXT: Cluster Command line
	SUB CONTEXT: Cluster Web Server   

CONTEXT -- Training Annotation -- no input, all features output along with positive / negaitve (defined by user)
	Training gets defined input, but in server will have one input file specifying peptide status and will use that to dynamically generate input fasta file	
	SUB CONTEXT: Single CPU
	SUB CONTEXT: Cluster Command Line
	SUB CONTEXT: Web Server phase one	

CONTEXT -- Training benchmarking -- input from file, output is the same as input. 
	SUB CONTEXT: Single  CPU
	SUB CONTEXT: Web Server phase 2, run on single cluster CPU

CONTEXT -- Training benchmarking, peptides in memory -- no inpu, output is the same as above
	SUB CONTEXT: Single CPU
	Only context I haven't written yet -- will hold off for now	

CONTEXT -- SVM Application from file, read input, write output -- same attribute file as above
	Input is annotation file previously done; not sure if need fasta file or not. Output is new annotation file	
	SUB CONTEXT: Single CPU
	SUB CONTEXT: Cluster Command Line
	Consider not supporting for now



TRAINING FLOW
1. make annotation, write to output file (has positive / negative tag)
2. read back in -- this will be the cluster way, although command line could go directly from memory
3. not writing back out since they don't have to be scored -- although could. 
4. big decision will be whether to write status -- think I should. can have true postivie / negative and also predicte positive / negative -- so status will always be output for a peptide
will have context for single and multiple iterations
single assumes that someone else will come along and read all directories with result files
multiple will do it itself
however, won't output to its own directory-- just make a class, point it to a fiel, and say add result or something
then single can reuse the directory each time, multiple can have files from different directories


PLACES THAT CHANGE ACROSS TROMBONE / MAC
testPcssConig:
pcss_directory
dssp_command
run_psipred_command / run_disopred_command


internalConfig:
home-pcss_dir
svm learn
svm classify


WEB SERVER:

Exception guidelines
if I am expecting a source file that I am copying or moving to exist, check to make sure it does and throw feature/protein exception if not; then run shutil (don't rely on that for exception)

for each exception:
make sure it is the right kind of exception
make sure there is a test for it or if not notate it needs one
make sure that features get peptides and proteins and fullproteins get protein somewhere along the way





	
Input / error / mismatch flow

Annotation OR SVM Application
Scan: If no peptides parsed, then won't process that protein anywhere and will just bring it along to the end and write the error in annotation file
      If no peptides parsed for all proteins, still should be fine, will just show up in the error column
Defined: If mismatch for one peptide, then just note it in the log in the front end 
	 need to figure out how to handle mismatch coming in from standalone
	 IF mismatch for all peptides, throw exception (already in there) since this shouldn't have been in there in the first place

Training Benchmark
Gets input by reading something that was already validated. Can't be worried about trying to validate it again at this point
However does do validation on more negatives than positives and test set positive count > 0
	Front end should check that too. No need to check up front in command line, it is the user's responsibility

Status: either positive, negative, or application. In Annotatin, file attribute is set to 'none' so that there isn't a non-sensible status column written out
but it can still be set as a feature


50 sequences
25000 = 500 runs / directories
50 sequence * 1 hour for disopred + 1 hour for psipred
4 hours for longest one, 3 hours for second longest







UPDATE TESTS WHEN ANNOTATION FILE CHANGES (for example, new column)

runners:
make sure old test is working
test_runners -- test_annotation_runner, copy output to testInput/svmApplicationAnnotationInput.txt
test_runners -- test_training_annotation_runner, copy output to testInput/svmTrainingAnnotationInput.txt
test all runners, uncomment pcssTests.compareToExpectedOutput line self.createNewExpectedOutputFiles
in newExpectedOutputFiles directory, run copy script to copy to exepcted output
comment createNewExpectedOutputFiles again
final test

svm:
copy annotation output file to svmErrors/nonStandardAaAnnotation.txt; change one peptide to X
copy training annotation output file to svmErrors/trainingMorePositives.txt; do emacs find and replace to create more positives than negatives
copy training annotatin output file to svmErrors/trainingNoTestSetPositives.txt; count through 8 positives and at least as many negatives, truncate the rest of the file
there are also two expected output files for test_svm that need to be re-created using copyFile.sh

readInput:
missingInputColumn.txt -- should be fine no matter what changes because looks for missing ModbaseSequenceID
extraInputColumn.txt -- copy annotation output file to ioErrors/extraInputColumn.txt; open, add fake column name and tab somewhere, close
noAnnotationProteins.txt -- copy annotation output file to ioErrors/noAnnotationProteins.txt and delete everything below column header
annotationOutputFeatureError.txt -- this one is tough since everything has to be 'no_source_model' -- think here it's best just to change it in excel to add the extra
column, or some other manual way depending on where the column is. Probably usually have to add values for the columns too or read / write will complain, even if there is
something in the error column.

cluster runners:
no real choice but to re-run on cluster. this is ok since that forces basic functionality check on there anyway.
go to cluster, run prepareSvmApplication
submit svm application cluster shell script
run finalize svm application
go to cluster run directory
scp -r seqBatchList dbarkan@trombone:pcss/test/testInput/cluster/
scp annotationOutput.txt dbarkan@trombone:pcss/test/testInput/expectedOutput/clusterRunner/finalizeSvmApplication_expectedOutput.txt


